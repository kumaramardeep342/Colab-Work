{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5FYNtQp+cyi4SUiKDmiYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumaramardeep342/Colab-Work/blob/main/Edureka_AI__NLP_M2_CaseStudy1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Write a program to enter a string from user and perform following tasks"
      ],
      "metadata": {
        "id": "K41OHsv7D1OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Write a python function named “Tokenize”which returns the tokenized string\n",
        "- Print tokens along with the frequency of each token using the above function\n",
        "- Print the 5 least occurring tokens"
      ],
      "metadata": {
        "id": "FCL_-a9ND97d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyNv6WNuHu8o",
        "outputId": "318ce8b7-fea9-4728-c9c6-42513bf235d3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "#sample_text = input(\"Enter a string: \")\n",
        "sample_text =  'Sun Pharmaceutical Industries Ltd on Tuesday said it will acquire a 16.33 per cent stake in Surgimatix, Inc, a US-based firm for $3.05 million (over Rs 25 crore). Sun Pharmaceutical has signed a definitive merger agreement for the acquisition of all outstanding ordinary shares of Taro Pharmaceutical not currently owned by the company or its affiliates for $347.73m (Rs28. 92bn) in cash.'\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenize(sample_text)\n",
        "print(f\"tokenized string : {tokens}\")\n",
        "\n",
        "# Calculate the frequency of each token\n",
        "fdist = FreqDist()\n",
        "for token in tokens:\n",
        "    fdist[token.lower()] += 1\n",
        "\n",
        "# Print the tokens and their frequencies\n",
        "# for token,frequency in fdist.items():\n",
        "#   print(f\"{token}:{frequency}\")\n",
        "print(f\"tokens and their frequencies : {fdist.items()}\")\n",
        "\n",
        "# Print the 5 least occurring tokens\n",
        "#print(fdist.most_common())\n",
        "print(\"5 least occurring tokens:\")\n",
        "for token, frequency in fdist.most_common()[:-6:-1]:\n",
        "    print(f\"{token}: {frequency}\")\n",
        "#print(fdist.most_common(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7i9hotJI5p9",
        "outputId": "65c52252-b3de-4b03-dffa-8d87e3e2a339"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized string : ['Sun', 'Pharmaceutical', 'Industries', 'Ltd', 'on', 'Tuesday', 'said', 'it', 'will', 'acquire', 'a', '16.33', 'per', 'cent', 'stake', 'in', 'Surgimatix', ',', 'Inc', ',', 'a', 'US-based', 'firm', 'for', '$', '3.05', 'million', '(', 'over', 'Rs', '25', 'crore', ')', '.', 'Sun', 'Pharmaceutical', 'has', 'signed', 'a', 'definitive', 'merger', 'agreement', 'for', 'the', 'acquisition', 'of', 'all', 'outstanding', 'ordinary', 'shares', 'of', 'Taro', 'Pharmaceutical', 'not', 'currently', 'owned', 'by', 'the', 'company', 'or', 'its', 'affiliates', 'for', '$', '347.73m', '(', 'Rs28', '.', '92bn', ')', 'in', 'cash', '.']\n",
            "tokens and their frequencies : dict_items([('sun', 2), ('pharmaceutical', 3), ('industries', 1), ('ltd', 1), ('on', 1), ('tuesday', 1), ('said', 1), ('it', 1), ('will', 1), ('acquire', 1), ('a', 3), ('16.33', 1), ('per', 1), ('cent', 1), ('stake', 1), ('in', 2), ('surgimatix', 1), (',', 2), ('inc', 1), ('us-based', 1), ('firm', 1), ('for', 3), ('$', 2), ('3.05', 1), ('million', 1), ('(', 2), ('over', 1), ('rs', 1), ('25', 1), ('crore', 1), (')', 2), ('.', 3), ('has', 1), ('signed', 1), ('definitive', 1), ('merger', 1), ('agreement', 1), ('the', 2), ('acquisition', 1), ('of', 2), ('all', 1), ('outstanding', 1), ('ordinary', 1), ('shares', 1), ('taro', 1), ('not', 1), ('currently', 1), ('owned', 1), ('by', 1), ('company', 1), ('or', 1), ('its', 1), ('affiliates', 1), ('347.73m', 1), ('rs28', 1), ('92bn', 1), ('cash', 1)])\n",
            "5 least occurring tokens:\n",
            "cash: 1\n",
            "92bn: 1\n",
            "rs28: 1\n",
            "347.73m: 1\n",
            "affiliates: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Write a program to enter a string from user and perform following tasks."
      ],
      "metadata": {
        "id": "pG6NvRKHEJN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Write a python functionnamed “RemoveStopWords”which returns the string after removing stop words\n",
        "- Count frequency ofeach stop word present in a string using the above function\n",
        "- Plot a bar graph depicting stop wordsand their frequencies"
      ],
      "metadata": {
        "id": "fMQA_QFzEQ0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Write a program to enter a string from user and perform following tasks"
      ],
      "metadata": {
        "id": "yEcbXENYEcZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Write a python functionnamed “Lemmatize”which returns a string after lemmatizing the string.\n",
        "- Write a python functionnamed “Stemmed” which returns a string after stemming the string. (Use any stemmer of your preference)\n",
        "- Print all the words along with their lemmatized and stemmed form using theabove functions\n",
        "- Save these results in a csv file having 3 columns:Original Word Lemmatized Form Stemmed Form"
      ],
      "metadata": {
        "id": "xSsTsFLZEisj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Create a python file named “PreProcess” and perform the following tasks."
      ],
      "metadata": {
        "id": "cn5QT5obFEld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Copy the function “Tokenize” in this file from question 1\n",
        "- Copy thefunction “RemoveStopWords” in this file from question 2\n",
        "- Copy the function “Lemmatize” in this file from question 3\n",
        "\n",
        "Create a function named “Refine” which accepts a string and call the above 3 functions in the same order i.e. first Tokenize then RemoveStopWords then Lemmatize.\n",
        "\n",
        "Remember:\n",
        "\n",
        "            >Inputted string will be input to Tokenize Function\n",
        "\n",
        "            > Tokenized String will be input to RemoveStopWords function\n",
        "\n",
        "            > StopWordsRemoved string will be input to Lemmatize function\n",
        "\n",
        "  Save this python file as PreProcess and you can use this for upcoming assignments."
      ],
      "metadata": {
        "id": "ON8xqP_QFMXT"
      }
    }
  ]
}